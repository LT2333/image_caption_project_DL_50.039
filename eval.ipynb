{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=128, caption_path='../datasets/coco2014/trainval_coco2014_captions/captions_val2014.json', decoder_path='models/my-decoder-5-3000-t4.ckpt', embed_size=256, encoder_path='models/my-encoder-5-3000-t4.ckpt', hidden_size=512, image_dir='data/resizedVal2014', log_step=1, num_epochs=5, num_layers=1, num_workers=2, save_step=1000, vocab_path='data/vocab_stemmed_t4.pkl')\n",
      "loading annotations into memory...\n",
      "Done (t=0.35s)\n",
      "creating index...\n",
      "index created!\n",
      "val_loader length = 1584\n",
      "step 0/1584, time 0m 1s, accuracy 0.5580720092915215, bleu_score 0.00.00.00.0\n",
      "['<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>']\n",
      "[['<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>', '<start>']]\n",
      "0.0078125 0.0078125 0.0078125 0.0078125\n",
      "val_loss = 0.001\n",
      "Cumulative 1-gram: 0.000005\n",
      "Cumulative 2-gram: 0.000005\n",
      "Cumulative 3-gram: 0.000005\n",
      "Cumulative 4-gram: 0.000005\n",
      "accuracy:  0.00035231818768404133\n"
     ]
    }
   ],
   "source": [
    "! python evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7860753021519787"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog']]\n",
    "candidate = ['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', 'from', 'space']\n",
    "sentence_bleu(reference, candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=128, caption_path='../datasets/coco2014/trainval_coco2014_captions/captions_val2014.json', decoder_path='models/my-decoder-5-3000-t4.ckpt', embed_size=256, encoder_path='models/my-encoder-5-3000-t4.ckpt', hidden_size=512, image_dir='data/resizedVal2014', log_step=5, num_epochs=5, num_layers=1, num_workers=2, save_step=1000, vocab_path='data/vocab_stemmed_t4.pkl')\n",
      "********Start Evaluation********\n",
      "loading annotations into memory...\n",
      "Done (t=0.36s)\n",
      "creating index...\n",
      "index created!\n",
      "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "candidate: ['front', 'game', 'around', 'a', 'corner', 'to', 'a', 'that', 'on', 'utensil', 'a', 'to', 'down', 'of']\n",
      "referrence [['front', 'game', 'around', 'a', 'anywher', 'with', 'round', 'on', 'at', 'knive', 'a', 'to', 'down', 'of']]\n",
      "Step [1/1584], Loss: 2.1223,BLEU score:0.241970,Accuracy:0.539648, Perplexity: 8.3505\n",
      "candidate: ['open', 'on', 'on', 'a', 'white', 'are', 'the', 'of', 'on', 'of', 'field', 'park', 'larg', 'in']\n",
      "referrence [['side', 'on', 'down', 'a', '<unk>', 'are', 'flight', 'wall', 'to', 'of', 'field', 'park', 'plane', 'in']]\n",
      "Step [6/1584], Loss: 2.2788,BLEU score:0.218130,Accuracy:0.533472, Perplexity: 9.7648\n",
      "candidate: ['field', 'and', 'tenni', 'up', 'of', 'her', 'play', 'fli', 'lay', 'lake', 'anim', 'a', 'skateboard']\n",
      "referrence [['field', 'with', 'tenni', 'up', 'in', 'a', 'perform', 'hold', 'rest', 'man', 'bear', 'a', 'skate']]\n",
      "Step [11/1584], Loss: 2.0952,BLEU score:0.207712,Accuracy:0.529830, Perplexity: 8.1268\n",
      "candidate: ['in', 'with', 'bed', 'plate', 'restroom', 'to', 'larg', 'and', 'bird', 'couch', 'to', 'the', 'numer']\n",
      "referrence [['in', 'with', 'motel', 'plate', 'bathroom', 'to', 'big', 'lay', 'cell', 'chair', 'to', 'to', 'numer']]\n",
      "Step [16/1584], Loss: 2.0554,BLEU score:0.207229,Accuracy:0.534563, Perplexity: 7.8097\n",
      "candidate: ['car', 'to', 'plate', 'and', 'horn', 'on', 'clock', 'on', 'from', 'couch', 'feet', 'black', 'traffic']\n",
      "referrence [['station', 'to', 'plate', ',', 'apparatus', 'travel', 'pretti', 'at', 'from', 'bed', 'breakfast', 'steepl', 'traffic']]\n",
      "Step [21/1584], Loss: 2.1070,BLEU score:0.208020,Accuracy:0.537527, Perplexity: 8.2232\n",
      "candidate: ['child', 'a', ',', 'as', 'a', 'are', 'around', 'food', 'differ', 'sit', 'dog', 'sit', 'a']\n",
      "referrence [['child', 'a', ',', 'for', 'his', 'has', 'near', 'tofu', 'uniform', 'in', 'dog', 'sit', 'a']]\n",
      "Step [26/1584], Loss: 2.2867,BLEU score:0.208258,Accuracy:0.536138, Perplexity: 9.8421\n",
      "candidate: ['to', 'wave', 'bathroom', 'a', 'on', 'a', 'trick', 'a', 'a', 'with', 'on', 'on', 'from', 'down']\n",
      "referrence [['to', 'wave', 'toilet', 'differ', 'on', 'a', 'trick', 'an', 'a', 'hold', 'outsid', 'on', 'from', 'with']]\n",
      "Step [31/1584], Loss: 2.1207,BLEU score:0.207143,Accuracy:0.536924, Perplexity: 8.3366\n",
      "candidate: ['on', 'park', 'a', 'a', 'each', 'motorcycl', ',', 'toilet', 'field', 'tooth', 'cloth', 'and', 'larg']\n",
      "referrence [['on', 'is', 'a', 'a', 'each', 'hot', ',', 'tile', 'field', 'toothbrush', 'beverag', 'sandwich', '<unk>']]\n",
      "Step [36/1584], Loss: 2.1281,BLEU score:0.207785,Accuracy:0.537717, Perplexity: 8.3993\n",
      "candidate: ['on', 'shirt', 'with', 'umbrella', 'with', 'girl', 'a', 'in', 'racket', 'of', 'swim', 'by', 'side']\n",
      "referrence [['in', 'dress', 'with', 'shirt', 'in', 'polic', 'the', 'on', 'racquet', 'of', 'lay', 'by', 'side']]\n",
      "Step [41/1584], Loss: 2.0889,BLEU score:0.209255,Accuracy:0.539293, Perplexity: 8.0763\n",
      "candidate: ['sit', 'ski', 'and', 'green', 'wooden', 'on', 'electr', 'from', 'it', 'a', 'on', 'a', 'are']\n",
      "referrence [['on', 'rope', 'in', 'field', '<unk>', 'on', 'electr', 'some', 'it', 'an', 'in', 'desk', 'is']]\n",
      "Step [46/1584], Loss: 2.0462,BLEU score:0.208625,Accuracy:0.539625, Perplexity: 7.7382\n",
      "candidate: ['a', 'a', 'next', 'in', 'drive', 'airport', 'stand', 'of', 'jean', 'tabl', 'food', 'and', 'stand']\n",
      "referrence [['a', 'white', 'fill', 'in', 'work', 'walkway', 'hit', 'of', 'one', 'stack', 'sandwich', 'and', 'net']]\n",
      "Step [51/1584], Loss: 2.1251,BLEU score:0.210163,Accuracy:0.540901, Perplexity: 8.3736\n",
      "candidate: ['doorway', 'head', 'sit', 'and', 'a', 'plate', 'in', 'water', 'the', 'a', 'with', 'the', 'a', 'grass']\n",
      "referrence [['tunnel', \"'s\", 'with', 'and', 'a', 'sandwich', 'on', 'water', 'the', 'a', 'road', 'a', 'a', 'foreground']]\n",
      "Step [56/1584], Loss: 2.2732,BLEU score:0.211306,Accuracy:0.540966, Perplexity: 9.7107\n",
      "candidate: ['sit', 'basebal', 'in', 'chair', 'bike', 'a', 'giraff', 'player', 'stand', 'on', 'top', 'citi', 'tie']\n",
      "referrence [['next', 'basebal', 'while', 'tabl', 'larg', 'a', 'bear', 'team', 'stand', 'with', 'gray', 'street', 'hat']]\n",
      "Step [61/1584], Loss: 2.0504,BLEU score:0.211595,Accuracy:0.541213, Perplexity: 7.7708\n",
      "candidate: ['next', 'fill', 'from', 'in', 'on', 'blue', 'beard', 'in', 'through', 'trick', 'racquet', 'glass', 'plate', 'a']\n",
      "referrence [['on', 'fill', 'while', 'stand', 'on', 'dress', 'stuf', 'near', 'that', 'trick', 'racquet', 'cone', 'red', 'a']]\n",
      "Step [66/1584], Loss: 2.1212,BLEU score:0.211981,Accuracy:0.541872, Perplexity: 8.3412\n",
      "candidate: ['for', 'of', 'ceil', 'hotel', 'around', 'the', 'lake', 'with', 'sit', 'and', 'trick', 'the', 'field']\n",
      "referrence [['the', 'of', 'ceil', 'hotel', 'in', 'the', 'larg', 'with', 'sit', 'next', 'a', 'top', 'field']]\n",
      "Step [71/1584], Loss: 2.0465,BLEU score:0.211898,Accuracy:0.542190, Perplexity: 7.7408\n",
      "candidate: ['go', 'a', 'on', 'up', 'on', 'are', 'on', 'couch', 'a', 'surfboard', 'around', 'bike', 'shown', 'water']\n",
      "referrence [['perform', 'two', 'do', 'up', 'up', 'have', 'near', 'fireplac', 'the', 'surfboard', 'outsid', 'bicycl', 'post', 'and']]\n",
      "Step [76/1584], Loss: 2.1079,BLEU score:0.211852,Accuracy:0.542441, Perplexity: 8.2310\n",
      "candidate: ['sleep', 'on', 'in', 'bear', 'a', 'red', 'dog', 'bench', 'on', 'and', 'hold', 'a', 'signal', 'boat']\n",
      "referrence [['sit', 'and', 'in', 'bear', 'a', 'long', 'lab', 'bench', 'with', 'and', 'is', 'bed', 'track', 'umbrella']]\n",
      "Step [81/1584], Loss: 2.0511,BLEU score:0.212331,Accuracy:0.542858, Perplexity: 7.7767\n",
      "candidate: ['sit', 'a', 'basebal', 'with', 'on', 'hit', ',', 'bridg', 'bag', 'of', 'in', 'a', 'a']\n",
      "referrence [['lay', 'a', 'guy', 'full', 'on', 'hit', 'and', 'river', 'luggag', 'of', 'lead', 'a', 'a']]\n",
      "Step [86/1584], Loss: 1.9928,BLEU score:0.212490,Accuracy:0.543221, Perplexity: 7.3358\n",
      "candidate: ['pull', 'potato', 'orang', 'a', 'umbrella', 'stand', 'red', 'collar', 'to', 'fruit', 'mound', 'a', 'up']\n",
      "referrence [['at', 'veget', 'orang', 'mani', 'open', 'stand', '<unk>', 'bow', 'to', 'fruit', 'mound', 'the', 'open']]\n",
      "Step [91/1584], Loss: 2.2121,BLEU score:0.212784,Accuracy:0.543605, Perplexity: 9.1353\n",
      "candidate: ['kite', 'side', 'down', 'the', 'of', 'on', 'is', 'after', 'a', 'her', 'street', 'front', 'sit']\n",
      "referrence [['kite', 'side', 'down', 'the', 'of', 'next', 'has', 'stand', 'a', 'the', 'green', 'font', 'gather']]\n",
      "Step [96/1584], Loss: 2.0314,BLEU score:0.212843,Accuracy:0.543731, Perplexity: 7.6244\n",
      "candidate: ['wall', ',', ',', 'a', 'in', 'runway', 'at', 'hind', 'bathroom', 'on', 'curtain', 'water', 'stand']\n",
      "referrence [['wall', 'and', ',', 'a', 'next', 'park', 'behind', 'back', 'jacket', 'on', 'stall', 'end', 'near']]\n",
      "Step [101/1584], Loss: 2.0374,BLEU score:0.213148,Accuracy:0.544085, Perplexity: 7.6710\n",
      "candidate: ['tabl', 'taken', 'top', 'glass', 'on', 'of', 'side', 'red', 'girl', 'keyboard', 'of', 'down', 'live']\n",
      "referrence [['kitchen', 'taken', 'top', 'glass', 'hold', 'in', 'side', 'blue', 'girl', '<unk>', 'of', 'through', 'pillar']]\n",
      "Step [106/1584], Loss: 2.0746,BLEU score:0.213600,Accuracy:0.544046, Perplexity: 7.9615\n",
      "candidate: ['a', 'in', 'a', 'path', 'lot', 'stand', 'bat', 'a', 'hang', 'open', 'peopl', 'cabinet', 'on', 'a']\n",
      "referrence [['peopl', 'in', 'a', 'trail', 'bus', 'shown', 'glove', 'cement', 'hang', 'umbrella', 'peopl', 'wardrob', 'on', 'a']]\n",
      "Step [111/1584], Loss: 2.0882,BLEU score:0.213408,Accuracy:0.544001, Perplexity: 8.0701\n",
      "candidate: ['across', 'board', 'airplan', 'game', 'from', 'stand', 'street', 'travel', 'item', 'wooden', 'to', 'cloth', 'a']\n",
      "referrence [['along', 'board', 'airplan', 'game', 'with', 'on', 'roadway', 'arriv', 'box', 'chair', 'to', 'tape', 'a']]\n",
      "Step [116/1584], Loss: 2.1391,BLEU score:0.212891,Accuracy:0.543593, Perplexity: 8.4918\n",
      "candidate: ['plate', 'on', 'shop', 'of', 'a', 'a', 'in', 'tabl', 'eat', 'hydrant', 'leav', 'stand', 'station']\n",
      "referrence [['plate', 'pile', 'up', 'are', 'slice', 'the', 'in', 'tabl', 'eat', 'hydrant', 'food', 'who', 'station']]\n",
      "Step [121/1584], Loss: 2.1629,BLEU score:0.213025,Accuracy:0.543658, Perplexity: 8.6964\n",
      "candidate: ['to', 'and', 'suit', 'sit', 'a', 'in', 'surf', 'out', 'to', 'cell', 'on', 'in', 'in', 'bench']\n",
      "referrence [['to', 'pose', 'suit', 'in', 'a', 'is', 'sand', 'from', 'as', 'cellular', 'by', 'next', 'and', 'chair']]\n",
      "Step [126/1584], Loss: 2.1420,BLEU score:0.213481,Accuracy:0.543487, Perplexity: 8.5163\n",
      "candidate: ['with', 'on', 'a', 'phone', 'the', 'tile', 'play', 'a', 'a', 'on', 'next', 'the', ',', 'harbor']\n",
      "referrence [['with', 'ride', 'black', 'cell', 'street', 'floor', 'tri', 'a', 'the', 'on', 'in', 'some', 'and', 'harbor']]\n",
      "Step [131/1584], Loss: 2.1432,BLEU score:0.213281,Accuracy:0.543437, Perplexity: 8.5267\n",
      "candidate: ['near', 'sink', 'on', 'tub', ',', 'front', 'kite', 'laptop', 'small', 'street', 'with', 'ocean', 'kitchen']\n",
      "referrence [[',', 'sink', ',', 'white', ',', 'front', 'dragon', 'comput', 'kitchen', 'cross', 'set', 'ocean', 'kitchen']]\n",
      "Step [136/1584], Loss: 2.1032,BLEU score:0.213271,Accuracy:0.543271, Perplexity: 8.1923\n",
      "candidate: ['are', 'the', 'book', 'eat', 'by', 'on', 'a', 'on', 'bat', 'and', 'air', 'on', 'in']\n",
      "referrence [['are', 'the', 'book', 'is', 'by', 'with', 'the', 'in', 'bat', 'serv', 'air', 'prepar', 'on']]\n",
      "Step [141/1584], Loss: 2.2464,BLEU score:0.213207,Accuracy:0.543113, Perplexity: 9.4533\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"evaluate2.py\", line 120, in <module>\n",
      "    main(args)\n",
      "  File \"evaluate2.py\", line 58, in main\n",
      "    for i, (images, captions, lengths) in enumerate(data_loader):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 345, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 841, in _next_data\n",
      "    idx, data = self._get_data()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 808, in _get_data\n",
      "    success, data = self._try_get_data()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 761, in _try_get_data\n",
      "    data = self._data_queue.get(timeout=timeout)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/conda/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! python evaluate2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=128, caption_path='../datasets/coco2014/trainval_coco2014_captions/captions_val2014.json', decoder_path='models/my-decoder-5-3000-t4.ckpt', embed_size=256, encoder_path='models/my-encoder-5-3000-t4.ckpt', hidden_size=512, image_dir='data/resizedVal2014', log_step=10, num_epochs=5, num_layers=1, num_workers=2, save_step=1000, vocab_path='data/vocab_stemmed_t4.pkl')\n",
      "********Start Evaluation********\n",
      "loading annotations into memory...\n",
      "Done (t=0.36s)\n",
      "creating index...\n",
      "index created!\n",
      "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Step [1/1584], Loss: 2.0774,BLEU score:0.565701/0.411484/0.295009/0.223513,Accuracy:0.552392, Perplexity: 7.9840\n",
      "Step [11/1584], Loss: 2.0251,BLEU score:0.579571/0.421720/0.304732/0.224964,Accuracy:0.552951, Perplexity: 7.5770\n",
      "Step [21/1584], Loss: 1.9693,BLEU score:0.576411/0.417376/0.301074/0.221772,Accuracy:0.550295, Perplexity: 7.1659\n",
      "Step [31/1584], Loss: 2.1815,BLEU score:0.574632/0.415692/0.298820/0.218248,Accuracy:0.547102, Perplexity: 8.8596\n",
      "Step [41/1584], Loss: 2.1410,BLEU score:0.572993/0.413686/0.297530/0.217584,Accuracy:0.545444, Perplexity: 8.5081\n",
      "Step [51/1584], Loss: 2.0989,BLEU score:0.572795/0.413352/0.297083/0.217276,Accuracy:0.544973, Perplexity: 8.1572\n",
      "Step [61/1584], Loss: 2.1477,BLEU score:0.572586/0.412603/0.295691/0.216179,Accuracy:0.544830, Perplexity: 8.5651\n",
      "Step [71/1584], Loss: 2.1024,BLEU score:0.571931/0.411690/0.294392/0.214674,Accuracy:0.544209, Perplexity: 8.1862\n",
      "Step [81/1584], Loss: 2.1015,BLEU score:0.572128/0.411792/0.293909/0.213985,Accuracy:0.544322, Perplexity: 8.1787\n",
      "Step [91/1584], Loss: 2.1996,BLEU score:0.572407/0.412198/0.293666/0.213506,Accuracy:0.544805, Perplexity: 9.0216\n",
      "Step [101/1584], Loss: 2.2632,BLEU score:0.571887/0.411017/0.292322/0.212160,Accuracy:0.543908, Perplexity: 9.6135\n",
      "Step [111/1584], Loss: 2.2678,BLEU score:0.571163/0.410078/0.291321/0.211251,Accuracy:0.543366, Perplexity: 9.6578\n",
      "Step [121/1584], Loss: 1.9996,BLEU score:0.571019/0.410060/0.291217/0.211307,Accuracy:0.543420, Perplexity: 7.3859\n",
      "Step [131/1584], Loss: 1.9409,BLEU score:0.570706/0.410057/0.291239/0.211545,Accuracy:0.543037, Perplexity: 6.9650\n",
      "Step [141/1584], Loss: 2.2797,BLEU score:0.570612/0.409837/0.290787/0.211215,Accuracy:0.542822, Perplexity: 9.7739\n",
      "Step [151/1584], Loss: 2.1305,BLEU score:0.570738/0.409853/0.291196/0.211579,Accuracy:0.542894, Perplexity: 8.4191\n",
      "Step [161/1584], Loss: 2.2723,BLEU score:0.570686/0.409683/0.291080/0.211571,Accuracy:0.542708, Perplexity: 9.7014\n",
      "Step [171/1584], Loss: 2.2045,BLEU score:0.570310/0.409309/0.290675/0.211377,Accuracy:0.542516, Perplexity: 9.0661\n",
      "Step [181/1584], Loss: 2.0780,BLEU score:0.570259/0.409484/0.290825/0.211395,Accuracy:0.542508, Perplexity: 7.9886\n",
      "Step [191/1584], Loss: 2.0166,BLEU score:0.570192/0.409603/0.290852/0.211479,Accuracy:0.542560, Perplexity: 7.5128\n",
      "Step [201/1584], Loss: 2.0202,BLEU score:0.570067/0.409291/0.290459/0.211169,Accuracy:0.542286, Perplexity: 7.5398\n",
      "Step [211/1584], Loss: 2.0918,BLEU score:0.570050/0.409267/0.290584/0.211370,Accuracy:0.542383, Perplexity: 8.0995\n",
      "Step [221/1584], Loss: 2.0459,BLEU score:0.570616/0.409747/0.290995/0.211708,Accuracy:0.542831, Perplexity: 7.7360\n",
      "Step [231/1584], Loss: 1.9533,BLEU score:0.570569/0.409632/0.290811/0.211574,Accuracy:0.542773, Perplexity: 7.0517\n",
      "Step [241/1584], Loss: 2.1114,BLEU score:0.570642/0.409768/0.290844/0.211610,Accuracy:0.542718, Perplexity: 8.2595\n",
      "Step [251/1584], Loss: 2.1566,BLEU score:0.570502/0.409766/0.290757/0.211626,Accuracy:0.542632, Perplexity: 8.6417\n",
      "Step [261/1584], Loss: 2.1447,BLEU score:0.570336/0.409599/0.290688/0.211517,Accuracy:0.542500, Perplexity: 8.5399\n",
      "Step [271/1584], Loss: 2.2424,BLEU score:0.570416/0.409761/0.290919/0.211651,Accuracy:0.542598, Perplexity: 9.4156\n",
      "Step [281/1584], Loss: 2.0899,BLEU score:0.570247/0.409438/0.290587/0.211363,Accuracy:0.542475, Perplexity: 8.0840\n",
      "Step [291/1584], Loss: 2.1748,BLEU score:0.570396/0.409596/0.290784/0.211563,Accuracy:0.542631, Perplexity: 8.8008\n",
      "Step [301/1584], Loss: 1.9948,BLEU score:0.570238/0.409368/0.290544/0.211408,Accuracy:0.542424, Perplexity: 7.3506\n",
      "Step [311/1584], Loss: 2.0359,BLEU score:0.570412/0.409619/0.290841/0.211707,Accuracy:0.542565, Perplexity: 7.6589\n",
      "Step [321/1584], Loss: 2.1362,BLEU score:0.570570/0.409849/0.291120/0.211973,Accuracy:0.542693, Perplexity: 8.4672\n",
      "Step [331/1584], Loss: 2.0965,BLEU score:0.570538/0.409851/0.291107/0.211990,Accuracy:0.542717, Perplexity: 8.1378\n",
      "Step [341/1584], Loss: 2.0128,BLEU score:0.570489/0.409812/0.291057/0.211865,Accuracy:0.542670, Perplexity: 7.4844\n",
      "Step [351/1584], Loss: 2.1802,BLEU score:0.570493/0.409870/0.291145/0.211964,Accuracy:0.542733, Perplexity: 8.8482\n",
      "Step [361/1584], Loss: 2.1301,BLEU score:0.570507/0.409915/0.291235/0.211996,Accuracy:0.542726, Perplexity: 8.4156\n",
      "Step [371/1584], Loss: 2.1311,BLEU score:0.570396/0.409731/0.291065/0.211845,Accuracy:0.542607, Perplexity: 8.4244\n",
      "Step [381/1584], Loss: 2.0024,BLEU score:0.570534/0.409870/0.291131/0.211931,Accuracy:0.542678, Perplexity: 7.4069\n",
      "Step [391/1584], Loss: 2.0334,BLEU score:0.570504/0.409800/0.290974/0.211770,Accuracy:0.542628, Perplexity: 7.6399\n",
      "Step [401/1584], Loss: 2.1827,BLEU score:0.570560/0.409784/0.291029/0.211905,Accuracy:0.542664, Perplexity: 8.8702\n",
      "Step [411/1584], Loss: 2.0511,BLEU score:0.570541/0.409714/0.290954/0.211811,Accuracy:0.542630, Perplexity: 7.7767\n",
      "Step [421/1584], Loss: 1.9981,BLEU score:0.570683/0.409834/0.291094/0.211891,Accuracy:0.542729, Perplexity: 7.3752\n",
      "Step [431/1584], Loss: 2.0180,BLEU score:0.570710/0.409899/0.291264/0.211942,Accuracy:0.542732, Perplexity: 7.5230\n",
      "Step [441/1584], Loss: 2.0487,BLEU score:0.570696/0.409853/0.291223/0.211895,Accuracy:0.542656, Perplexity: 7.7580\n",
      "Step [451/1584], Loss: 2.1139,BLEU score:0.570792/0.410006/0.291338/0.212030,Accuracy:0.542736, Perplexity: 8.2801\n",
      "Step [461/1584], Loss: 1.9908,BLEU score:0.570850/0.410026/0.291329/0.212008,Accuracy:0.542762, Perplexity: 7.3215\n",
      "Step [471/1584], Loss: 2.0395,BLEU score:0.570785/0.409936/0.291199/0.211885,Accuracy:0.542729, Perplexity: 7.6868\n",
      "Step [481/1584], Loss: 2.1057,BLEU score:0.570725/0.409956/0.291242/0.211944,Accuracy:0.542717, Perplexity: 8.2127\n",
      "Step [491/1584], Loss: 2.0604,BLEU score:0.570797/0.409979/0.291231/0.211997,Accuracy:0.542731, Perplexity: 7.8488\n",
      "Step [501/1584], Loss: 2.0900,BLEU score:0.570840/0.409924/0.291230/0.212012,Accuracy:0.542743, Perplexity: 8.0848\n",
      "Step [511/1584], Loss: 2.0440,BLEU score:0.570885/0.409986/0.291316/0.212163,Accuracy:0.542788, Perplexity: 7.7213\n",
      "Step [521/1584], Loss: 2.0836,BLEU score:0.570978/0.410140/0.291484/0.212298,Accuracy:0.542895, Perplexity: 8.0331\n",
      "Step [531/1584], Loss: 2.2381,BLEU score:0.570933/0.410109/0.291414/0.212167,Accuracy:0.542863, Perplexity: 9.3754\n",
      "Step [541/1584], Loss: 2.0500,BLEU score:0.570900/0.410081/0.291316/0.212054,Accuracy:0.542832, Perplexity: 7.7680\n",
      "Step [551/1584], Loss: 2.1297,BLEU score:0.570852/0.410000/0.291251/0.211988,Accuracy:0.542803, Perplexity: 8.4125\n",
      "Step [561/1584], Loss: 1.9351,BLEU score:0.570913/0.410120/0.291371/0.212077,Accuracy:0.542868, Perplexity: 6.9248\n",
      "Step [571/1584], Loss: 2.1501,BLEU score:0.570794/0.410057/0.291371/0.212056,Accuracy:0.542783, Perplexity: 8.5861\n",
      "Step [581/1584], Loss: 1.9914,BLEU score:0.570857/0.410119/0.291421/0.212100,Accuracy:0.542885, Perplexity: 7.3258\n",
      "Step [591/1584], Loss: 2.1481,BLEU score:0.570754/0.410014/0.291436/0.212124,Accuracy:0.542796, Perplexity: 8.5682\n",
      "Step [601/1584], Loss: 2.1541,BLEU score:0.570635/0.409897/0.291326/0.212086,Accuracy:0.542697, Perplexity: 8.6203\n",
      "Step [611/1584], Loss: 2.0351,BLEU score:0.570515/0.409817/0.291205/0.211946,Accuracy:0.542572, Perplexity: 7.6529\n",
      "Step [621/1584], Loss: 1.9736,BLEU score:0.570528/0.409847/0.291262/0.211995,Accuracy:0.542594, Perplexity: 7.1969\n",
      "Step [631/1584], Loss: 1.9267,BLEU score:0.570623/0.409951/0.291375/0.212066,Accuracy:0.542699, Perplexity: 6.8666\n",
      "Step [641/1584], Loss: 2.1225,BLEU score:0.570569/0.409932/0.291362/0.212024,Accuracy:0.542641, Perplexity: 8.3521\n",
      "Step [651/1584], Loss: 2.0032,BLEU score:0.570520/0.409812/0.291222/0.211917,Accuracy:0.542613, Perplexity: 7.4129\n",
      "Step [661/1584], Loss: 2.1654,BLEU score:0.570513/0.409825/0.291215/0.211956,Accuracy:0.542591, Perplexity: 8.7178\n",
      "Step [671/1584], Loss: 1.9881,BLEU score:0.570498/0.409776/0.291131/0.211853,Accuracy:0.542608, Perplexity: 7.3017\n",
      "Step [681/1584], Loss: 2.0120,BLEU score:0.570480/0.409732/0.291104/0.211830,Accuracy:0.542605, Perplexity: 7.4782\n",
      "Step [691/1584], Loss: 2.0115,BLEU score:0.570473/0.409732/0.291035/0.211761,Accuracy:0.542575, Perplexity: 7.4749\n",
      "Step [701/1584], Loss: 2.0602,BLEU score:0.570542/0.409837/0.291217/0.211821,Accuracy:0.542650, Perplexity: 7.8473\n",
      "Step [711/1584], Loss: 2.0642,BLEU score:0.570551/0.409847/0.291211/0.211848,Accuracy:0.542649, Perplexity: 7.8793\n",
      "Step [721/1584], Loss: 1.9663,BLEU score:0.570632/0.409918/0.291299/0.211898,Accuracy:0.542704, Perplexity: 7.1443\n",
      "Step [731/1584], Loss: 2.1294,BLEU score:0.570609/0.409868/0.291251/0.211832,Accuracy:0.542714, Perplexity: 8.4095\n",
      "Step [741/1584], Loss: 2.0202,BLEU score:0.570628/0.409920/0.291312/0.211930,Accuracy:0.542721, Perplexity: 7.5400\n",
      "Step [751/1584], Loss: 2.2137,BLEU score:0.570554/0.409819/0.291227/0.211794,Accuracy:0.542646, Perplexity: 9.1495\n",
      "Step [761/1584], Loss: 2.1265,BLEU score:0.570558/0.409837/0.291243/0.211874,Accuracy:0.542654, Perplexity: 8.3852\n",
      "Step [771/1584], Loss: 2.0103,BLEU score:0.570608/0.409927/0.291279/0.211898,Accuracy:0.542700, Perplexity: 7.4657\n",
      "Step [781/1584], Loss: 2.1842,BLEU score:0.570540/0.409851/0.291218/0.211850,Accuracy:0.542645, Perplexity: 8.8833\n",
      "Step [791/1584], Loss: 2.1180,BLEU score:0.570604/0.409914/0.291268/0.211860,Accuracy:0.542709, Perplexity: 8.3147\n",
      "Step [801/1584], Loss: 2.1051,BLEU score:0.570641/0.409947/0.291336/0.211947,Accuracy:0.542747, Perplexity: 8.2075\n",
      "Step [811/1584], Loss: 2.1483,BLEU score:0.570652/0.409940/0.291350/0.212016,Accuracy:0.542744, Perplexity: 8.5705\n",
      "Step [821/1584], Loss: 2.0932,BLEU score:0.570698/0.409987/0.291385/0.212078,Accuracy:0.542816, Perplexity: 8.1109\n",
      "Step [831/1584], Loss: 2.1635,BLEU score:0.570640/0.409894/0.291238/0.211903,Accuracy:0.542756, Perplexity: 8.7016\n",
      "Step [841/1584], Loss: 1.9487,BLEU score:0.570562/0.409811/0.291150/0.211841,Accuracy:0.542683, Perplexity: 7.0192\n",
      "Step [851/1584], Loss: 2.1304,BLEU score:0.570508/0.409736/0.291036/0.211817,Accuracy:0.542689, Perplexity: 8.4180\n",
      "Step [861/1584], Loss: 2.0760,BLEU score:0.570529/0.409714/0.291008/0.211775,Accuracy:0.542683, Perplexity: 7.9724\n",
      "Step [871/1584], Loss: 1.9897,BLEU score:0.570599/0.409740/0.291047/0.211756,Accuracy:0.542755, Perplexity: 7.3131\n",
      "Step [881/1584], Loss: 2.0866,BLEU score:0.570613/0.409729/0.291080/0.211771,Accuracy:0.542747, Perplexity: 8.0576\n",
      "Step [891/1584], Loss: 2.0582,BLEU score:0.570699/0.409750/0.291094/0.211820,Accuracy:0.542787, Perplexity: 7.8316\n",
      "Step [901/1584], Loss: 2.0837,BLEU score:0.570772/0.409839/0.291164/0.211888,Accuracy:0.542850, Perplexity: 8.0341\n",
      "Step [911/1584], Loss: 1.9893,BLEU score:0.570777/0.409805/0.291097/0.211830,Accuracy:0.542826, Perplexity: 7.3106\n",
      "Step [921/1584], Loss: 2.1022,BLEU score:0.570833/0.409878/0.291117/0.211855,Accuracy:0.542855, Perplexity: 8.1838\n",
      "Step [931/1584], Loss: 2.1568,BLEU score:0.570881/0.410002/0.291201/0.211954,Accuracy:0.542910, Perplexity: 8.6435\n",
      "Step [941/1584], Loss: 2.1073,BLEU score:0.570889/0.410027/0.291199/0.211883,Accuracy:0.542927, Perplexity: 8.2262\n",
      "Step [951/1584], Loss: 2.0794,BLEU score:0.570908/0.410027/0.291165/0.211839,Accuracy:0.542968, Perplexity: 8.0000\n",
      "Step [961/1584], Loss: 2.1175,BLEU score:0.570910/0.410041/0.291200/0.211851,Accuracy:0.542987, Perplexity: 8.3105\n",
      "Step [971/1584], Loss: 2.0590,BLEU score:0.570876/0.409996/0.291147/0.211787,Accuracy:0.542973, Perplexity: 7.8381\n",
      "Step [981/1584], Loss: 2.0507,BLEU score:0.570813/0.409966/0.291099/0.211743,Accuracy:0.542930, Perplexity: 7.7731\n",
      "Step [991/1584], Loss: 2.1871,BLEU score:0.570862/0.410023/0.291169/0.211787,Accuracy:0.542958, Perplexity: 8.9090\n",
      "Step [1001/1584], Loss: 2.1040,BLEU score:0.570892/0.410079/0.291230/0.211813,Accuracy:0.542981, Perplexity: 8.1989\n",
      "Step [1011/1584], Loss: 2.0811,BLEU score:0.570865/0.410052/0.291215/0.211817,Accuracy:0.542948, Perplexity: 8.0134\n",
      "Step [1021/1584], Loss: 2.2162,BLEU score:0.570946/0.410181/0.291379/0.211957,Accuracy:0.543054, Perplexity: 9.1722\n",
      "Step [1031/1584], Loss: 2.0208,BLEU score:0.570944/0.410172/0.291379/0.211990,Accuracy:0.543034, Perplexity: 7.5445\n",
      "Step [1041/1584], Loss: 2.0107,BLEU score:0.570887/0.410087/0.291317/0.211978,Accuracy:0.543001, Perplexity: 7.4683\n",
      "Step [1051/1584], Loss: 2.1315,BLEU score:0.570850/0.410026/0.291262/0.211908,Accuracy:0.542962, Perplexity: 8.4278\n",
      "Step [1061/1584], Loss: 2.0834,BLEU score:0.570780/0.409984/0.291257/0.211945,Accuracy:0.542883, Perplexity: 8.0319\n",
      "Step [1071/1584], Loss: 2.1144,BLEU score:0.570756/0.409973/0.291272/0.211928,Accuracy:0.542857, Perplexity: 8.2846\n",
      "Step [1081/1584], Loss: 2.0957,BLEU score:0.570760/0.409960/0.291247/0.211899,Accuracy:0.542865, Perplexity: 8.1308\n",
      "Step [1091/1584], Loss: 1.9695,BLEU score:0.570778/0.409976/0.291271/0.211937,Accuracy:0.542859, Perplexity: 7.1672\n",
      "Step [1101/1584], Loss: 2.0923,BLEU score:0.570765/0.409964/0.291242/0.211914,Accuracy:0.542833, Perplexity: 8.1034\n",
      "Step [1111/1584], Loss: 2.0349,BLEU score:0.570758/0.409945/0.291220/0.211906,Accuracy:0.542815, Perplexity: 7.6517\n",
      "Step [1121/1584], Loss: 2.0615,BLEU score:0.570734/0.409901/0.291147/0.211825,Accuracy:0.542811, Perplexity: 7.8576\n",
      "Step [1131/1584], Loss: 2.1157,BLEU score:0.570658/0.409795/0.291027/0.211737,Accuracy:0.542736, Perplexity: 8.2955\n",
      "Step [1141/1584], Loss: 2.1876,BLEU score:0.570595/0.409692/0.290944/0.211669,Accuracy:0.542716, Perplexity: 8.9137\n",
      "Step [1151/1584], Loss: 2.1677,BLEU score:0.570602/0.409696/0.290949/0.211685,Accuracy:0.542728, Perplexity: 8.7382\n",
      "Step [1161/1584], Loss: 1.9641,BLEU score:0.570620/0.409721/0.290968/0.211685,Accuracy:0.542746, Perplexity: 7.1286\n",
      "Step [1171/1584], Loss: 2.1768,BLEU score:0.570604/0.409726/0.291008/0.211749,Accuracy:0.542766, Perplexity: 8.8183\n",
      "Step [1181/1584], Loss: 2.0009,BLEU score:0.570652/0.409769/0.291045/0.211775,Accuracy:0.542817, Perplexity: 7.3957\n",
      "Step [1191/1584], Loss: 1.9463,BLEU score:0.570702/0.409818/0.291113/0.211867,Accuracy:0.542846, Perplexity: 7.0026\n",
      "Step [1201/1584], Loss: 2.0775,BLEU score:0.570728/0.409870/0.291204/0.211963,Accuracy:0.542882, Perplexity: 7.9846\n",
      "Step [1211/1584], Loss: 1.9625,BLEU score:0.570757/0.409892/0.291256/0.212000,Accuracy:0.542883, Perplexity: 7.1174\n",
      "Step [1221/1584], Loss: 2.2439,BLEU score:0.570764/0.409887/0.291261/0.212060,Accuracy:0.542887, Perplexity: 9.4301\n",
      "Step [1231/1584], Loss: 2.1684,BLEU score:0.570795/0.409940/0.291317/0.212128,Accuracy:0.542918, Perplexity: 8.7440\n",
      "Step [1241/1584], Loss: 2.2541,BLEU score:0.570750/0.409902/0.291320/0.212128,Accuracy:0.542881, Perplexity: 9.5266\n",
      "Step [1251/1584], Loss: 2.2506,BLEU score:0.570731/0.409851/0.291269/0.212090,Accuracy:0.542850, Perplexity: 9.4935\n",
      "Step [1261/1584], Loss: 2.0335,BLEU score:0.570751/0.409892/0.291301/0.212130,Accuracy:0.542875, Perplexity: 7.6404\n",
      "Step [1271/1584], Loss: 2.1228,BLEU score:0.570791/0.409907/0.291304/0.212135,Accuracy:0.542906, Perplexity: 8.3546\n",
      "Step [1281/1584], Loss: 2.1102,BLEU score:0.570840/0.409943/0.291327/0.212122,Accuracy:0.542907, Perplexity: 8.2497\n",
      "Step [1291/1584], Loss: 2.1447,BLEU score:0.570826/0.409917/0.291291/0.212105,Accuracy:0.542896, Perplexity: 8.5395\n",
      "Step [1301/1584], Loss: 2.1530,BLEU score:0.570785/0.409900/0.291260/0.212070,Accuracy:0.542858, Perplexity: 8.6110\n",
      "Step [1311/1584], Loss: 2.0824,BLEU score:0.570770/0.409877/0.291254/0.212066,Accuracy:0.542870, Perplexity: 8.0233\n",
      "Step [1321/1584], Loss: 2.0621,BLEU score:0.570737/0.409838/0.291209/0.212010,Accuracy:0.542837, Perplexity: 7.8623\n",
      "Step [1331/1584], Loss: 1.8740,BLEU score:0.570719/0.409800/0.291162/0.211964,Accuracy:0.542829, Perplexity: 6.5143\n",
      "Step [1341/1584], Loss: 2.1480,BLEU score:0.570708/0.409749/0.291158/0.211975,Accuracy:0.542809, Perplexity: 8.5680\n",
      "Step [1351/1584], Loss: 2.2286,BLEU score:0.570704/0.409714/0.291135/0.211979,Accuracy:0.542808, Perplexity: 9.2869\n",
      "Step [1361/1584], Loss: 2.1395,BLEU score:0.570693/0.409703/0.291129/0.211998,Accuracy:0.542779, Perplexity: 8.4948\n",
      "Step [1371/1584], Loss: 2.0441,BLEU score:0.570679/0.409679/0.291085/0.211967,Accuracy:0.542773, Perplexity: 7.7219\n",
      "Step [1381/1584], Loss: 2.1624,BLEU score:0.570656/0.409667/0.291059/0.211928,Accuracy:0.542756, Perplexity: 8.6920\n",
      "Step [1391/1584], Loss: 2.2109,BLEU score:0.570682/0.409704/0.291101/0.211963,Accuracy:0.542771, Perplexity: 9.1235\n",
      "Step [1401/1584], Loss: 1.9759,BLEU score:0.570752/0.409778/0.291179/0.212002,Accuracy:0.542836, Perplexity: 7.2129\n",
      "Step [1411/1584], Loss: 2.1449,BLEU score:0.570743/0.409775/0.291158/0.211975,Accuracy:0.542821, Perplexity: 8.5410\n",
      "Step [1421/1584], Loss: 2.1286,BLEU score:0.570714/0.409715/0.291101/0.211920,Accuracy:0.542805, Perplexity: 8.4027\n",
      "Step [1431/1584], Loss: 2.1685,BLEU score:0.570731/0.409731/0.291107/0.211953,Accuracy:0.542816, Perplexity: 8.7447\n",
      "Step [1441/1584], Loss: 2.0610,BLEU score:0.570682/0.409702/0.291068/0.211917,Accuracy:0.542771, Perplexity: 7.8535\n",
      "Step [1451/1584], Loss: 2.0342,BLEU score:0.570701/0.409748/0.291133/0.211978,Accuracy:0.542765, Perplexity: 7.6459\n",
      "Step [1461/1584], Loss: 2.0158,BLEU score:0.570718/0.409773/0.291152/0.211988,Accuracy:0.542788, Perplexity: 7.5064\n",
      "Step [1471/1584], Loss: 2.0349,BLEU score:0.570704/0.409766/0.291117/0.211951,Accuracy:0.542783, Perplexity: 7.6514\n",
      "Step [1481/1584], Loss: 2.0804,BLEU score:0.570720/0.409761/0.291133/0.211977,Accuracy:0.542794, Perplexity: 8.0075\n",
      "Step [1491/1584], Loss: 2.0697,BLEU score:0.570694/0.409749/0.291142/0.211993,Accuracy:0.542774, Perplexity: 7.9225\n",
      "Step [1501/1584], Loss: 2.1472,BLEU score:0.570712/0.409762/0.291166/0.212009,Accuracy:0.542802, Perplexity: 8.5610\n",
      "Step [1511/1584], Loss: 2.1649,BLEU score:0.570755/0.409794/0.291212/0.212048,Accuracy:0.542833, Perplexity: 8.7141\n",
      "Step [1521/1584], Loss: 2.1737,BLEU score:0.570799/0.409864/0.291277/0.212102,Accuracy:0.542884, Perplexity: 8.7908\n",
      "Step [1531/1584], Loss: 2.1740,BLEU score:0.570811/0.409861/0.291281/0.212091,Accuracy:0.542889, Perplexity: 8.7931\n",
      "Step [1541/1584], Loss: 2.1767,BLEU score:0.570792/0.409849/0.291283/0.212098,Accuracy:0.542870, Perplexity: 8.8174\n",
      "Step [1551/1584], Loss: 2.0998,BLEU score:0.570826/0.409876/0.291325/0.212120,Accuracy:0.542888, Perplexity: 8.1646\n",
      "Step [1561/1584], Loss: 2.0969,BLEU score:0.570810/0.409883/0.291339/0.212111,Accuracy:0.542895, Perplexity: 8.1407\n",
      "Step [1571/1584], Loss: 2.0166,BLEU score:0.570846/0.409939/0.291420/0.212155,Accuracy:0.542937, Perplexity: 7.5129\n",
      "Step [1581/1584], Loss: 2.0549,BLEU score:0.570818/0.409930/0.291367/0.212085,Accuracy:0.542914, Perplexity: 7.8058\n"
     ]
    }
   ],
   "source": [
    "# 这个是 resnet50 + treshold=4 +batchnorm \n",
    "! python evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=128, caption_path='../datasets/coco2014/trainval_coco2014_captions/captions_val2014.json', decoder_path='models/my-decoder-5-2000-t10.ckpt', embed_size=256, encoder_path='models/my-encoder-5-2000-t10.ckpt', hidden_size=512, image_dir='data/resizedVal2014', log_step=10, num_epochs=5, num_layers=1, num_workers=2, save_step=1000, vocab_path='data/vocab_stemmed_t10.pkl')\n",
      "********Start Evaluation********\n",
      "loading annotations into memory...\n",
      "Done (t=0.36s)\n",
      "creating index...\n",
      "index created!\n",
      "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Step [1/1584], Loss: 2.2612,BLEU score:0.549120/0.385892/0.273506/0.191900,Accuracy:0.522170, Perplexity: 9.5947\n",
      "Step [11/1584], Loss: 2.0303,BLEU score:0.564685/0.403856/0.289182/0.212993,Accuracy:0.535565, Perplexity: 7.6165\n",
      "Step [21/1584], Loss: 2.1293,BLEU score:0.564079/0.405011/0.288636/0.213872,Accuracy:0.535894, Perplexity: 8.4086\n",
      "Step [31/1584], Loss: 1.9262,BLEU score:0.565894/0.406504/0.290014/0.213893,Accuracy:0.536858, Perplexity: 6.8632\n",
      "Step [41/1584], Loss: 2.0901,BLEU score:0.568317/0.408727/0.291724/0.214916,Accuracy:0.539345, Perplexity: 8.0856\n",
      "Step [51/1584], Loss: 2.1025,BLEU score:0.567509/0.407227/0.290206/0.213524,Accuracy:0.539049, Perplexity: 8.1867\n",
      "Step [61/1584], Loss: 2.0667,BLEU score:0.568240/0.408565/0.292115/0.215114,Accuracy:0.539948, Perplexity: 7.8988\n",
      "Step [71/1584], Loss: 2.1892,BLEU score:0.568152/0.408865/0.292147/0.214691,Accuracy:0.540271, Perplexity: 8.9282\n",
      "Step [81/1584], Loss: 2.1904,BLEU score:0.568486/0.408881/0.292254/0.214048,Accuracy:0.540580, Perplexity: 8.9389\n",
      "Step [91/1584], Loss: 2.1316,BLEU score:0.568810/0.409127/0.291848/0.213490,Accuracy:0.540615, Perplexity: 8.4279\n",
      "Step [101/1584], Loss: 2.0942,BLEU score:0.569285/0.409215/0.291791/0.213387,Accuracy:0.540739, Perplexity: 8.1192\n",
      "Step [111/1584], Loss: 1.9889,BLEU score:0.569145/0.409087/0.291510/0.212795,Accuracy:0.540480, Perplexity: 7.3075\n",
      "Step [121/1584], Loss: 2.1344,BLEU score:0.568600/0.408545/0.290594/0.211904,Accuracy:0.540155, Perplexity: 8.4516\n",
      "Step [131/1584], Loss: 2.1539,BLEU score:0.568480/0.408598/0.290741/0.211752,Accuracy:0.540211, Perplexity: 8.6186\n",
      "Step [141/1584], Loss: 1.9928,BLEU score:0.568828/0.408706/0.290846/0.211860,Accuracy:0.540391, Perplexity: 7.3363\n",
      "Step [151/1584], Loss: 2.1777,BLEU score:0.569252/0.409199/0.291365/0.212439,Accuracy:0.540611, Perplexity: 8.8263\n",
      "Step [161/1584], Loss: 2.1818,BLEU score:0.569176/0.409111/0.291237/0.212495,Accuracy:0.540598, Perplexity: 8.8623\n",
      "Step [171/1584], Loss: 2.0618,BLEU score:0.569123/0.408879/0.290926/0.212100,Accuracy:0.540585, Perplexity: 7.8598\n",
      "Step [181/1584], Loss: 2.1909,BLEU score:0.568998/0.408608/0.290686/0.211821,Accuracy:0.540630, Perplexity: 8.9435\n",
      "Step [191/1584], Loss: 2.0563,BLEU score:0.569215/0.408935/0.290845/0.212177,Accuracy:0.540845, Perplexity: 7.8173\n",
      "Step [201/1584], Loss: 2.1378,BLEU score:0.569324/0.409104/0.291020/0.212472,Accuracy:0.540951, Perplexity: 8.4806\n",
      "Step [211/1584], Loss: 1.9651,BLEU score:0.569446/0.409032/0.290946/0.212384,Accuracy:0.540925, Perplexity: 7.1353\n",
      "Step [221/1584], Loss: 2.0062,BLEU score:0.569816/0.409383/0.291340/0.212729,Accuracy:0.541281, Perplexity: 7.4348\n",
      "Step [231/1584], Loss: 2.0121,BLEU score:0.569962/0.409561/0.291397/0.212661,Accuracy:0.541368, Perplexity: 7.4788\n",
      "Step [241/1584], Loss: 2.0193,BLEU score:0.570101/0.409707/0.291727/0.212780,Accuracy:0.541546, Perplexity: 7.5327\n",
      "Step [251/1584], Loss: 2.1632,BLEU score:0.570102/0.409567/0.291631/0.212620,Accuracy:0.541460, Perplexity: 8.6985\n",
      "Step [261/1584], Loss: 1.9530,BLEU score:0.570089/0.409578/0.291773/0.212696,Accuracy:0.541408, Perplexity: 7.0500\n",
      "Step [271/1584], Loss: 2.1558,BLEU score:0.569998/0.409511/0.291521/0.212303,Accuracy:0.541322, Perplexity: 8.6350\n",
      "Step [281/1584], Loss: 2.1246,BLEU score:0.570087/0.409510/0.291614/0.212308,Accuracy:0.541439, Perplexity: 8.3692\n",
      "Step [291/1584], Loss: 2.2201,BLEU score:0.570077/0.409529/0.291644/0.212485,Accuracy:0.541493, Perplexity: 9.2082\n",
      "Step [301/1584], Loss: 2.0486,BLEU score:0.569837/0.409408/0.291473/0.212476,Accuracy:0.541347, Perplexity: 7.7573\n",
      "Step [311/1584], Loss: 2.0491,BLEU score:0.569632/0.409126/0.291199/0.212190,Accuracy:0.541171, Perplexity: 7.7609\n",
      "Step [321/1584], Loss: 2.0327,BLEU score:0.569755/0.409278/0.291183/0.212211,Accuracy:0.541292, Perplexity: 7.6349\n",
      "Step [331/1584], Loss: 1.9485,BLEU score:0.569615/0.409154/0.290960/0.211913,Accuracy:0.541238, Perplexity: 7.0181\n",
      "Step [341/1584], Loss: 2.1853,BLEU score:0.569479/0.408892/0.290691/0.211621,Accuracy:0.541128, Perplexity: 8.8935\n",
      "Step [351/1584], Loss: 2.0573,BLEU score:0.569508/0.408891/0.290587/0.211535,Accuracy:0.541141, Perplexity: 7.8246\n",
      "Step [361/1584], Loss: 2.0797,BLEU score:0.569536/0.408830/0.290507/0.211510,Accuracy:0.541145, Perplexity: 8.0017\n",
      "Step [371/1584], Loss: 2.1267,BLEU score:0.569607/0.408881/0.290660/0.211619,Accuracy:0.541166, Perplexity: 8.3872\n",
      "Step [381/1584], Loss: 2.0323,BLEU score:0.569667/0.408999/0.290817/0.211723,Accuracy:0.541198, Perplexity: 7.6315\n",
      "Step [391/1584], Loss: 2.0383,BLEU score:0.569807/0.409150/0.290936/0.211728,Accuracy:0.541317, Perplexity: 7.6778\n",
      "Step [401/1584], Loss: 2.0471,BLEU score:0.569796/0.409075/0.290897/0.211702,Accuracy:0.541268, Perplexity: 7.7457\n",
      "Step [411/1584], Loss: 2.0118,BLEU score:0.569999/0.409280/0.291075/0.211823,Accuracy:0.541454, Perplexity: 7.4765\n",
      "Step [421/1584], Loss: 2.0365,BLEU score:0.570153/0.409476/0.291303/0.212057,Accuracy:0.541581, Perplexity: 7.6636\n",
      "Step [431/1584], Loss: 1.8995,BLEU score:0.570255/0.409552/0.291366/0.212075,Accuracy:0.541624, Perplexity: 6.6824\n",
      "Step [441/1584], Loss: 1.9877,BLEU score:0.570239/0.409522/0.291362/0.212135,Accuracy:0.541603, Perplexity: 7.2987\n",
      "Step [451/1584], Loss: 1.9615,BLEU score:0.570309/0.409552/0.291366/0.212077,Accuracy:0.541678, Perplexity: 7.1098\n",
      "Step [461/1584], Loss: 2.0614,BLEU score:0.570148/0.409425/0.291289/0.212031,Accuracy:0.541560, Perplexity: 7.8568\n",
      "Step [471/1584], Loss: 1.9172,BLEU score:0.570150/0.409456/0.291421/0.212131,Accuracy:0.541576, Perplexity: 6.8022\n",
      "Step [481/1584], Loss: 2.1296,BLEU score:0.570103/0.409409/0.291396/0.212159,Accuracy:0.541477, Perplexity: 8.4116\n",
      "Step [491/1584], Loss: 2.0119,BLEU score:0.570140/0.409538/0.291507/0.212291,Accuracy:0.541544, Perplexity: 7.4774\n",
      "Step [501/1584], Loss: 2.1771,BLEU score:0.570186/0.409570/0.291493/0.212328,Accuracy:0.541602, Perplexity: 8.8207\n",
      "Step [511/1584], Loss: 2.1143,BLEU score:0.570118/0.409442/0.291371/0.212205,Accuracy:0.541543, Perplexity: 8.2841\n",
      "Step [521/1584], Loss: 2.1722,BLEU score:0.570005/0.409321/0.291318/0.212166,Accuracy:0.541439, Perplexity: 8.7772\n",
      "Step [531/1584], Loss: 2.0324,BLEU score:0.570152/0.409430/0.291370/0.212235,Accuracy:0.541572, Perplexity: 7.6326\n",
      "Step [541/1584], Loss: 2.1743,BLEU score:0.570158/0.409431/0.291378/0.212196,Accuracy:0.541601, Perplexity: 8.7962\n",
      "Step [551/1584], Loss: 2.1491,BLEU score:0.570094/0.409447/0.291370/0.212171,Accuracy:0.541573, Perplexity: 8.5773\n",
      "Step [561/1584], Loss: 2.1218,BLEU score:0.569998/0.409293/0.291211/0.212037,Accuracy:0.541512, Perplexity: 8.3464\n",
      "Step [571/1584], Loss: 2.1416,BLEU score:0.569926/0.409149/0.291007/0.211919,Accuracy:0.541370, Perplexity: 8.5129\n",
      "Step [581/1584], Loss: 2.0510,BLEU score:0.569942/0.409134/0.290969/0.211917,Accuracy:0.541404, Perplexity: 7.7757\n",
      "Step [591/1584], Loss: 2.1496,BLEU score:0.569895/0.409022/0.290825/0.211773,Accuracy:0.541344, Perplexity: 8.5817\n",
      "Step [601/1584], Loss: 2.0085,BLEU score:0.569788/0.408943/0.290712/0.211740,Accuracy:0.541293, Perplexity: 7.4518\n",
      "Step [611/1584], Loss: 2.0631,BLEU score:0.569928/0.409048/0.290838/0.211875,Accuracy:0.541404, Perplexity: 7.8705\n",
      "Step [621/1584], Loss: 2.0208,BLEU score:0.570000/0.409098/0.290823/0.211894,Accuracy:0.541440, Perplexity: 7.5444\n",
      "Step [631/1584], Loss: 2.1163,BLEU score:0.569896/0.408957/0.290648/0.211719,Accuracy:0.541334, Perplexity: 8.3007\n",
      "Step [641/1584], Loss: 1.9673,BLEU score:0.569829/0.408912/0.290560/0.211635,Accuracy:0.541265, Perplexity: 7.1512\n",
      "Step [651/1584], Loss: 2.0007,BLEU score:0.569814/0.408886/0.290633/0.211680,Accuracy:0.541244, Perplexity: 7.3942\n",
      "Step [661/1584], Loss: 2.2122,BLEU score:0.569751/0.408854/0.290638/0.211683,Accuracy:0.541200, Perplexity: 9.1359\n",
      "Step [671/1584], Loss: 2.0612,BLEU score:0.569806/0.408845/0.290646/0.211672,Accuracy:0.541229, Perplexity: 7.8554\n",
      "Step [681/1584], Loss: 2.1999,BLEU score:0.569838/0.408857/0.290661/0.211737,Accuracy:0.541300, Perplexity: 9.0244\n",
      "Step [691/1584], Loss: 2.0032,BLEU score:0.569942/0.408952/0.290809/0.211888,Accuracy:0.541369, Perplexity: 7.4127\n",
      "Step [701/1584], Loss: 2.0695,BLEU score:0.569854/0.408909/0.290777/0.211891,Accuracy:0.541317, Perplexity: 7.9211\n",
      "Step [711/1584], Loss: 2.0433,BLEU score:0.569910/0.408990/0.290902/0.211950,Accuracy:0.541377, Perplexity: 7.7162\n",
      "Step [721/1584], Loss: 2.0655,BLEU score:0.569976/0.409034/0.290909/0.211950,Accuracy:0.541439, Perplexity: 7.8890\n",
      "Step [731/1584], Loss: 2.1938,BLEU score:0.569951/0.409032/0.290899/0.212014,Accuracy:0.541384, Perplexity: 8.9692\n",
      "Step [741/1584], Loss: 2.0208,BLEU score:0.569923/0.409039/0.290980/0.212163,Accuracy:0.541370, Perplexity: 7.5443\n",
      "Step [751/1584], Loss: 2.2270,BLEU score:0.569885/0.409011/0.290912/0.212100,Accuracy:0.541346, Perplexity: 9.2722\n",
      "Step [761/1584], Loss: 2.0535,BLEU score:0.569839/0.408976/0.290853/0.212053,Accuracy:0.541307, Perplexity: 7.7950\n",
      "Step [771/1584], Loss: 1.9776,BLEU score:0.569821/0.408946/0.290797/0.211982,Accuracy:0.541290, Perplexity: 7.2257\n",
      "Step [781/1584], Loss: 2.2534,BLEU score:0.569886/0.409030/0.290887/0.212080,Accuracy:0.541350, Perplexity: 9.5204\n",
      "Step [791/1584], Loss: 2.0036,BLEU score:0.569930/0.409072/0.290950/0.212129,Accuracy:0.541371, Perplexity: 7.4156\n",
      "Step [801/1584], Loss: 2.1544,BLEU score:0.569952/0.409076/0.290982/0.212149,Accuracy:0.541401, Perplexity: 8.6225\n",
      "Step [811/1584], Loss: 2.0584,BLEU score:0.569961/0.409085/0.290993/0.212164,Accuracy:0.541402, Perplexity: 7.8336\n",
      "Step [821/1584], Loss: 2.0822,BLEU score:0.569986/0.409126/0.290998/0.212184,Accuracy:0.541413, Perplexity: 8.0225\n",
      "Step [831/1584], Loss: 2.1549,BLEU score:0.570065/0.409248/0.291112/0.212317,Accuracy:0.541496, Perplexity: 8.6268\n",
      "Step [841/1584], Loss: 2.0798,BLEU score:0.569948/0.409104/0.290989/0.212243,Accuracy:0.541386, Perplexity: 8.0026\n",
      "Step [851/1584], Loss: 2.1387,BLEU score:0.569927/0.409064/0.291010/0.212253,Accuracy:0.541368, Perplexity: 8.4883\n",
      "Step [861/1584], Loss: 2.1523,BLEU score:0.569877/0.409012/0.290955/0.212197,Accuracy:0.541356, Perplexity: 8.6046\n",
      "Step [871/1584], Loss: 2.0690,BLEU score:0.569884/0.409034/0.290972/0.212204,Accuracy:0.541399, Perplexity: 7.9166\n",
      "Step [881/1584], Loss: 1.9965,BLEU score:0.569870/0.409027/0.290953/0.212140,Accuracy:0.541408, Perplexity: 7.3630\n",
      "Step [891/1584], Loss: 2.0013,BLEU score:0.569819/0.408994/0.290916/0.212070,Accuracy:0.541352, Perplexity: 7.3989\n",
      "Step [901/1584], Loss: 2.0799,BLEU score:0.569785/0.408957/0.290905/0.212081,Accuracy:0.541344, Perplexity: 8.0040\n",
      "Step [911/1584], Loss: 1.9411,BLEU score:0.569864/0.409092/0.290993/0.212168,Accuracy:0.541426, Perplexity: 6.9665\n",
      "Step [921/1584], Loss: 2.0602,BLEU score:0.569869/0.409121/0.291011/0.212151,Accuracy:0.541437, Perplexity: 7.8474\n",
      "Step [931/1584], Loss: 2.0813,BLEU score:0.569834/0.409095/0.290964/0.212102,Accuracy:0.541414, Perplexity: 8.0150\n",
      "Step [941/1584], Loss: 2.2153,BLEU score:0.569750/0.409011/0.290904/0.212051,Accuracy:0.541345, Perplexity: 9.1639\n",
      "Step [951/1584], Loss: 2.0318,BLEU score:0.569787/0.409081/0.290963/0.212106,Accuracy:0.541375, Perplexity: 7.6281\n",
      "Step [961/1584], Loss: 2.0693,BLEU score:0.569809/0.409101/0.290961/0.212106,Accuracy:0.541380, Perplexity: 7.9191\n",
      "Step [971/1584], Loss: 1.9895,BLEU score:0.569802/0.409053/0.290955/0.212100,Accuracy:0.541393, Perplexity: 7.3122\n",
      "Step [981/1584], Loss: 2.0063,BLEU score:0.569913/0.409187/0.291091/0.212277,Accuracy:0.541508, Perplexity: 7.4357\n",
      "Step [991/1584], Loss: 2.1015,BLEU score:0.569987/0.409255/0.291185/0.212348,Accuracy:0.541591, Perplexity: 8.1782\n",
      "Step [1001/1584], Loss: 2.2566,BLEU score:0.569920/0.409156/0.291105/0.212314,Accuracy:0.541528, Perplexity: 9.5503\n",
      "Step [1011/1584], Loss: 2.1355,BLEU score:0.569872/0.409100/0.291048/0.212286,Accuracy:0.541504, Perplexity: 8.4612\n",
      "Step [1021/1584], Loss: 2.0066,BLEU score:0.569876/0.409084/0.291035/0.212309,Accuracy:0.541516, Perplexity: 7.4377\n",
      "Step [1031/1584], Loss: 2.0689,BLEU score:0.569894/0.409082/0.291045/0.212332,Accuracy:0.541532, Perplexity: 7.9164\n",
      "Step [1041/1584], Loss: 2.0590,BLEU score:0.569950/0.409152/0.291083/0.212311,Accuracy:0.541587, Perplexity: 7.8383\n",
      "Step [1051/1584], Loss: 2.1015,BLEU score:0.569949/0.409139/0.291053/0.212285,Accuracy:0.541601, Perplexity: 8.1788\n",
      "Step [1061/1584], Loss: 2.1066,BLEU score:0.569929/0.409085/0.291003/0.212256,Accuracy:0.541603, Perplexity: 8.2206\n",
      "Step [1071/1584], Loss: 2.1169,BLEU score:0.569946/0.409109/0.291016/0.212256,Accuracy:0.541629, Perplexity: 8.3057\n",
      "Step [1081/1584], Loss: 2.1181,BLEU score:0.569956/0.409125/0.291023/0.212263,Accuracy:0.541613, Perplexity: 8.3156\n",
      "Step [1091/1584], Loss: 2.1611,BLEU score:0.569935/0.409094/0.291013/0.212252,Accuracy:0.541604, Perplexity: 8.6806\n",
      "Step [1101/1584], Loss: 2.0264,BLEU score:0.569940/0.409057/0.290968/0.212210,Accuracy:0.541605, Perplexity: 7.5869\n",
      "Step [1111/1584], Loss: 2.0480,BLEU score:0.569924/0.409041/0.290966/0.212195,Accuracy:0.541583, Perplexity: 7.7522\n",
      "Step [1121/1584], Loss: 2.0903,BLEU score:0.569996/0.409117/0.291023/0.212214,Accuracy:0.541639, Perplexity: 8.0870\n",
      "Step [1131/1584], Loss: 2.0610,BLEU score:0.569933/0.409040/0.290978/0.212170,Accuracy:0.541566, Perplexity: 7.8541\n",
      "Step [1141/1584], Loss: 2.1284,BLEU score:0.569920/0.409022/0.290996/0.212211,Accuracy:0.541557, Perplexity: 8.4012\n",
      "Step [1151/1584], Loss: 1.9999,BLEU score:0.569935/0.409019/0.290977/0.212186,Accuracy:0.541581, Perplexity: 7.3885\n",
      "Step [1161/1584], Loss: 2.0706,BLEU score:0.569936/0.409033/0.290986/0.212189,Accuracy:0.541580, Perplexity: 7.9294\n",
      "Step [1171/1584], Loss: 2.0510,BLEU score:0.569971/0.409066/0.290983/0.212198,Accuracy:0.541610, Perplexity: 7.7759\n",
      "Step [1181/1584], Loss: 1.9402,BLEU score:0.569982/0.409058/0.290994/0.212200,Accuracy:0.541625, Perplexity: 6.9602\n",
      "Step [1191/1584], Loss: 1.9791,BLEU score:0.570023/0.409152/0.291101/0.212270,Accuracy:0.541688, Perplexity: 7.2366\n",
      "Step [1201/1584], Loss: 1.9895,BLEU score:0.570041/0.409157/0.291089/0.212211,Accuracy:0.541715, Perplexity: 7.3116\n",
      "Step [1211/1584], Loss: 2.0660,BLEU score:0.570002/0.409116/0.291054/0.212154,Accuracy:0.541708, Perplexity: 7.8934\n",
      "Step [1221/1584], Loss: 2.1486,BLEU score:0.570030/0.409134/0.291096/0.212184,Accuracy:0.541705, Perplexity: 8.5727\n",
      "Step [1231/1584], Loss: 2.0139,BLEU score:0.570027/0.409140/0.291100/0.212156,Accuracy:0.541696, Perplexity: 7.4923\n",
      "Step [1241/1584], Loss: 2.0621,BLEU score:0.570090/0.409205/0.291177/0.212257,Accuracy:0.541731, Perplexity: 7.8622\n",
      "Step [1251/1584], Loss: 2.1809,BLEU score:0.570061/0.409154/0.291129/0.212220,Accuracy:0.541691, Perplexity: 8.8543\n",
      "Step [1261/1584], Loss: 2.1056,BLEU score:0.570031/0.409138/0.291147/0.212242,Accuracy:0.541679, Perplexity: 8.2122\n",
      "Step [1271/1584], Loss: 1.9777,BLEU score:0.569990/0.409095/0.291107/0.212191,Accuracy:0.541660, Perplexity: 7.2261\n",
      "Step [1281/1584], Loss: 2.0215,BLEU score:0.569997/0.409104/0.291123/0.212204,Accuracy:0.541674, Perplexity: 7.5499\n",
      "Step [1291/1584], Loss: 2.0652,BLEU score:0.570064/0.409170/0.291156/0.212215,Accuracy:0.541714, Perplexity: 7.8866\n",
      "Step [1301/1584], Loss: 1.9430,BLEU score:0.570099/0.409206/0.291150/0.212222,Accuracy:0.541747, Perplexity: 6.9798\n",
      "Step [1311/1584], Loss: 2.0315,BLEU score:0.570143/0.409298/0.291268/0.212290,Accuracy:0.541814, Perplexity: 7.6253\n",
      "Step [1321/1584], Loss: 2.0931,BLEU score:0.570148/0.409313/0.291278/0.212252,Accuracy:0.541818, Perplexity: 8.1101\n",
      "Step [1331/1584], Loss: 2.1117,BLEU score:0.570162/0.409327/0.291241/0.212212,Accuracy:0.541841, Perplexity: 8.2623\n",
      "Step [1341/1584], Loss: 2.1281,BLEU score:0.570131/0.409287/0.291202/0.212152,Accuracy:0.541799, Perplexity: 8.3985\n",
      "Step [1351/1584], Loss: 2.0739,BLEU score:0.570172/0.409337/0.291236/0.212168,Accuracy:0.541851, Perplexity: 7.9560\n",
      "Step [1361/1584], Loss: 2.0514,BLEU score:0.570150/0.409294/0.291172/0.212113,Accuracy:0.541844, Perplexity: 7.7788\n",
      "Step [1371/1584], Loss: 2.2074,BLEU score:0.570134/0.409285/0.291180/0.212130,Accuracy:0.541822, Perplexity: 9.0924\n",
      "Step [1381/1584], Loss: 2.2388,BLEU score:0.570092/0.409221/0.291111/0.212053,Accuracy:0.541791, Perplexity: 9.3819\n",
      "Step [1391/1584], Loss: 2.0819,BLEU score:0.570103/0.409251/0.291147/0.212080,Accuracy:0.541827, Perplexity: 8.0193\n",
      "Step [1401/1584], Loss: 2.0020,BLEU score:0.570122/0.409277/0.291175/0.212069,Accuracy:0.541847, Perplexity: 7.4039\n",
      "Step [1411/1584], Loss: 2.0945,BLEU score:0.570174/0.409357/0.291242/0.212103,Accuracy:0.541906, Perplexity: 8.1213\n",
      "Step [1421/1584], Loss: 1.9841,BLEU score:0.570166/0.409339/0.291260/0.212121,Accuracy:0.541882, Perplexity: 7.2726\n",
      "Step [1431/1584], Loss: 2.2031,BLEU score:0.570148/0.409330/0.291274/0.212143,Accuracy:0.541855, Perplexity: 9.0528\n",
      "Step [1441/1584], Loss: 2.1369,BLEU score:0.570155/0.409370/0.291313/0.212155,Accuracy:0.541873, Perplexity: 8.4730\n",
      "Step [1451/1584], Loss: 1.9817,BLEU score:0.570171/0.409373/0.291311/0.212138,Accuracy:0.541875, Perplexity: 7.2549\n",
      "Step [1461/1584], Loss: 2.0329,BLEU score:0.570171/0.409390/0.291345/0.212155,Accuracy:0.541883, Perplexity: 7.6362\n",
      "Step [1471/1584], Loss: 2.0938,BLEU score:0.570166/0.409372/0.291336/0.212176,Accuracy:0.541861, Perplexity: 8.1157\n",
      "Step [1481/1584], Loss: 2.0428,BLEU score:0.570204/0.409416/0.291403/0.212219,Accuracy:0.541894, Perplexity: 7.7120\n",
      "Step [1491/1584], Loss: 1.9928,BLEU score:0.570234/0.409464/0.291453/0.212279,Accuracy:0.541923, Perplexity: 7.3364\n",
      "Step [1501/1584], Loss: 2.1214,BLEU score:0.570210/0.409440/0.291423/0.212252,Accuracy:0.541901, Perplexity: 8.3428\n",
      "Step [1511/1584], Loss: 2.1632,BLEU score:0.570206/0.409407/0.291369/0.212250,Accuracy:0.541899, Perplexity: 8.6991\n",
      "Step [1521/1584], Loss: 2.0712,BLEU score:0.570195/0.409391/0.291381/0.212255,Accuracy:0.541884, Perplexity: 7.9341\n",
      "Step [1531/1584], Loss: 2.0344,BLEU score:0.570239/0.409429/0.291411/0.212297,Accuracy:0.541895, Perplexity: 7.6480\n",
      "Step [1541/1584], Loss: 2.0452,BLEU score:0.570219/0.409402/0.291396/0.212273,Accuracy:0.541867, Perplexity: 7.7310\n",
      "Step [1551/1584], Loss: 2.0649,BLEU score:0.570177/0.409348/0.291359/0.212261,Accuracy:0.541835, Perplexity: 7.8844\n",
      "Step [1561/1584], Loss: 2.0815,BLEU score:0.570171/0.409364/0.291387/0.212285,Accuracy:0.541832, Perplexity: 8.0165\n",
      "Step [1571/1584], Loss: 2.1155,BLEU score:0.570188/0.409384/0.291412/0.212341,Accuracy:0.541843, Perplexity: 8.2941\n",
      "Step [1581/1584], Loss: 2.2857,BLEU score:0.570178/0.409368/0.291380/0.212294,Accuracy:0.541817, Perplexity: 9.8330\n"
     ]
    }
   ],
   "source": [
    "# 这个是 resnet50 + treshold=10 +batchnorm \n",
    "! python evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=128, caption_path='../datasets/coco2014/trainval_coco2014_captions/captions_val2014.json', decoder_path='models/my-decoder-5-3000-t4-resnext.ckpt', embed_size=256, encoder_path='models/my-encoder-5-3000-t4-resnext.ckpt', hidden_size=512, image_dir='data/resizedVal2014', log_step=10, num_epochs=5, num_layers=1, num_workers=2, save_step=1000, vocab_path='data/vocab_stemmed_t4.pkl')\n",
      "********Start Evaluation********\n",
      "loading annotations into memory...\n",
      "Done (t=0.40s)\n",
      "creating index...\n",
      "index created!\n",
      "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/opt/conda/lib/python3.6/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "Step [1/1584], Loss: 2.0659,BLEU score:0.586171/0.426716/0.299706/0.225678,Accuracy:0.555938, Perplexity: 7.8923\n",
      "Step [11/1584], Loss: 2.0447,BLEU score:0.575238/0.417663/0.296757/0.215526,Accuracy:0.548695, Perplexity: 7.7267\n",
      "Step [21/1584], Loss: 2.0107,BLEU score:0.573285/0.415275/0.297775/0.216265,Accuracy:0.546330, Perplexity: 7.4688\n",
      "Step [31/1584], Loss: 2.0122,BLEU score:0.573829/0.415740/0.299043/0.219639,Accuracy:0.547312, Perplexity: 7.4794\n",
      "Step [41/1584], Loss: 2.0530,BLEU score:0.573495/0.414897/0.297955/0.218865,Accuracy:0.546487, Perplexity: 7.7916\n",
      "Step [51/1584], Loss: 2.0916,BLEU score:0.573368/0.414978/0.298278/0.218873,Accuracy:0.546094, Perplexity: 8.0977\n",
      "Step [61/1584], Loss: 1.9685,BLEU score:0.572869/0.414494/0.297597/0.218355,Accuracy:0.545644, Perplexity: 7.1600\n",
      "Step [71/1584], Loss: 2.3076,BLEU score:0.573529/0.414993/0.298222/0.219120,Accuracy:0.546298, Perplexity: 10.0500\n",
      "Step [81/1584], Loss: 2.2016,BLEU score:0.573807/0.415163/0.298109/0.219063,Accuracy:0.546453, Perplexity: 9.0395\n",
      "Step [91/1584], Loss: 2.0422,BLEU score:0.573375/0.414399/0.297286/0.218663,Accuracy:0.545487, Perplexity: 7.7072\n",
      "Step [101/1584], Loss: 1.9986,BLEU score:0.573508/0.414882/0.297492/0.218892,Accuracy:0.545778, Perplexity: 7.3786\n",
      "Step [111/1584], Loss: 2.0363,BLEU score:0.573631/0.414805/0.297619/0.218732,Accuracy:0.545901, Perplexity: 7.6623\n",
      "Step [121/1584], Loss: 2.0059,BLEU score:0.573102/0.413988/0.296749/0.217617,Accuracy:0.545253, Perplexity: 7.4325\n",
      "Step [131/1584], Loss: 2.0089,BLEU score:0.573163/0.414011/0.296897/0.217518,Accuracy:0.545282, Perplexity: 7.4549\n",
      "Step [141/1584], Loss: 2.0921,BLEU score:0.573081/0.413821/0.296583/0.217151,Accuracy:0.545129, Perplexity: 8.1016\n",
      "Step [151/1584], Loss: 2.0216,BLEU score:0.573519/0.414588/0.297133/0.217564,Accuracy:0.545698, Perplexity: 7.5507\n",
      "Step [161/1584], Loss: 1.9423,BLEU score:0.573426/0.414614/0.297222/0.217905,Accuracy:0.545784, Perplexity: 6.9750\n",
      "Step [171/1584], Loss: 1.9756,BLEU score:0.573148/0.414237/0.296684/0.217411,Accuracy:0.545455, Perplexity: 7.2108\n",
      "Step [181/1584], Loss: 2.0396,BLEU score:0.572981/0.413974/0.296646/0.217234,Accuracy:0.545234, Perplexity: 7.6872\n",
      "Step [191/1584], Loss: 2.0186,BLEU score:0.573108/0.414233/0.296957/0.217431,Accuracy:0.545436, Perplexity: 7.5275\n",
      "Step [201/1584], Loss: 2.1168,BLEU score:0.573266/0.414290/0.296767/0.217239,Accuracy:0.545659, Perplexity: 8.3043\n",
      "Step [211/1584], Loss: 2.0317,BLEU score:0.572890/0.413898/0.296374/0.217022,Accuracy:0.545241, Perplexity: 7.6272\n",
      "Step [221/1584], Loss: 2.1879,BLEU score:0.572551/0.413486/0.295897/0.216617,Accuracy:0.544860, Perplexity: 8.9160\n",
      "Step [231/1584], Loss: 1.9234,BLEU score:0.572247/0.413121/0.295542/0.216179,Accuracy:0.544578, Perplexity: 6.8442\n",
      "Step [241/1584], Loss: 1.9172,BLEU score:0.572204/0.413113/0.295356/0.216192,Accuracy:0.544559, Perplexity: 6.8021\n",
      "Step [251/1584], Loss: 2.0842,BLEU score:0.571840/0.412659/0.294811/0.215643,Accuracy:0.544151, Perplexity: 8.0383\n",
      "Step [261/1584], Loss: 2.1368,BLEU score:0.571638/0.412350/0.294425/0.215268,Accuracy:0.543960, Perplexity: 8.4723\n",
      "Step [271/1584], Loss: 2.0567,BLEU score:0.571559/0.412258/0.294346/0.215227,Accuracy:0.543934, Perplexity: 7.8203\n",
      "Step [281/1584], Loss: 2.0786,BLEU score:0.571602/0.412582/0.294793/0.215594,Accuracy:0.544082, Perplexity: 7.9935\n",
      "Step [291/1584], Loss: 2.0769,BLEU score:0.571482/0.412387/0.294694/0.215570,Accuracy:0.543975, Perplexity: 7.9793\n",
      "Step [301/1584], Loss: 2.0515,BLEU score:0.571460/0.412284/0.294574/0.215610,Accuracy:0.543923, Perplexity: 7.7792\n",
      "Step [311/1584], Loss: 2.0198,BLEU score:0.571402/0.412161/0.294457/0.215544,Accuracy:0.543905, Perplexity: 7.5365\n",
      "Step [321/1584], Loss: 2.1132,BLEU score:0.571514/0.412221/0.294551/0.215687,Accuracy:0.543980, Perplexity: 8.2745\n",
      "Step [331/1584], Loss: 2.0874,BLEU score:0.571441/0.412116/0.294472/0.215626,Accuracy:0.543933, Perplexity: 8.0643\n",
      "Step [341/1584], Loss: 2.1349,BLEU score:0.571226/0.411901/0.294203/0.215279,Accuracy:0.543794, Perplexity: 8.4564\n",
      "Step [351/1584], Loss: 1.9861,BLEU score:0.571323/0.411975/0.294205/0.215194,Accuracy:0.543893, Perplexity: 7.2868\n",
      "Step [361/1584], Loss: 2.1490,BLEU score:0.571325/0.411918/0.294190/0.215251,Accuracy:0.543854, Perplexity: 8.5762\n",
      "Step [371/1584], Loss: 2.1244,BLEU score:0.571075/0.411543/0.293840/0.214972,Accuracy:0.543612, Perplexity: 8.3680\n",
      "Step [381/1584], Loss: 2.0853,BLEU score:0.571134/0.411488/0.293871/0.214970,Accuracy:0.543601, Perplexity: 8.0467\n",
      "Step [391/1584], Loss: 2.0960,BLEU score:0.571175/0.411494/0.293852/0.214839,Accuracy:0.543626, Perplexity: 8.1334\n",
      "Step [401/1584], Loss: 2.0578,BLEU score:0.571230/0.411629/0.294005/0.215044,Accuracy:0.543653, Perplexity: 7.8286\n",
      "Step [411/1584], Loss: 2.0292,BLEU score:0.571286/0.411742/0.294187/0.215132,Accuracy:0.543740, Perplexity: 7.6079\n",
      "Step [421/1584], Loss: 2.1516,BLEU score:0.571272/0.411605/0.293918/0.214724,Accuracy:0.543650, Perplexity: 8.5990\n",
      "Step [431/1584], Loss: 2.1054,BLEU score:0.571267/0.411556/0.293923/0.214862,Accuracy:0.543603, Perplexity: 8.2105\n",
      "Step [441/1584], Loss: 2.0788,BLEU score:0.571373/0.411635/0.294022/0.214967,Accuracy:0.543641, Perplexity: 7.9948\n",
      "Step [451/1584], Loss: 2.1347,BLEU score:0.571404/0.411667/0.293990/0.215005,Accuracy:0.543641, Perplexity: 8.4544\n",
      "Step [461/1584], Loss: 2.0121,BLEU score:0.571416/0.411670/0.294050/0.215052,Accuracy:0.543639, Perplexity: 7.4788\n",
      "Step [471/1584], Loss: 2.2094,BLEU score:0.571357/0.411589/0.294051/0.215014,Accuracy:0.543564, Perplexity: 9.1103\n",
      "Step [481/1584], Loss: 2.0274,BLEU score:0.571310/0.411522/0.293919/0.214847,Accuracy:0.543541, Perplexity: 7.5941\n",
      "Step [491/1584], Loss: 2.0305,BLEU score:0.571330/0.411512/0.293855/0.214780,Accuracy:0.543550, Perplexity: 7.6182\n",
      "Step [501/1584], Loss: 2.0197,BLEU score:0.571231/0.411347/0.293681/0.214639,Accuracy:0.543435, Perplexity: 7.5360\n",
      "Step [511/1584], Loss: 2.0571,BLEU score:0.571180/0.411303/0.293652/0.214686,Accuracy:0.543392, Perplexity: 7.8234\n",
      "Step [521/1584], Loss: 2.2351,BLEU score:0.571192/0.411307/0.293663/0.214702,Accuracy:0.543396, Perplexity: 9.3476\n",
      "Step [531/1584], Loss: 2.0303,BLEU score:0.571111/0.411171/0.293504/0.214610,Accuracy:0.543340, Perplexity: 7.6160\n",
      "Step [541/1584], Loss: 2.0998,BLEU score:0.570962/0.411015/0.293380/0.214512,Accuracy:0.543202, Perplexity: 8.1648\n",
      "Step [551/1584], Loss: 2.1444,BLEU score:0.570929/0.411027/0.293384/0.214545,Accuracy:0.543157, Perplexity: 8.5372\n",
      "Step [561/1584], Loss: 2.0077,BLEU score:0.570988/0.411042/0.293450/0.214589,Accuracy:0.543219, Perplexity: 7.4462\n",
      "Step [571/1584], Loss: 2.0411,BLEU score:0.571116/0.411198/0.293640/0.214790,Accuracy:0.543364, Perplexity: 7.6994\n",
      "Step [581/1584], Loss: 2.0390,BLEU score:0.571041/0.411094/0.293539/0.214734,Accuracy:0.543341, Perplexity: 7.6830\n",
      "Step [591/1584], Loss: 2.0006,BLEU score:0.571048/0.411082/0.293463/0.214557,Accuracy:0.543333, Perplexity: 7.3934\n",
      "Step [601/1584], Loss: 2.1807,BLEU score:0.571094/0.411160/0.293605/0.214645,Accuracy:0.543360, Perplexity: 8.8523\n",
      "Step [611/1584], Loss: 2.1108,BLEU score:0.571101/0.411127/0.293596/0.214707,Accuracy:0.543386, Perplexity: 8.2551\n",
      "Step [621/1584], Loss: 2.0664,BLEU score:0.571172/0.411203/0.293674/0.214699,Accuracy:0.543474, Perplexity: 7.8961\n",
      "Step [631/1584], Loss: 2.0237,BLEU score:0.571185/0.411223/0.293717/0.214773,Accuracy:0.543494, Perplexity: 7.5664\n",
      "Step [641/1584], Loss: 2.1566,BLEU score:0.571073/0.411104/0.293568/0.214630,Accuracy:0.543380, Perplexity: 8.6421\n",
      "Step [651/1584], Loss: 2.0048,BLEU score:0.571111/0.411095/0.293572/0.214572,Accuracy:0.543390, Perplexity: 7.4245\n",
      "Step [661/1584], Loss: 2.0395,BLEU score:0.571198/0.411204/0.293691/0.214696,Accuracy:0.543491, Perplexity: 7.6870\n",
      "Step [671/1584], Loss: 2.2483,BLEU score:0.571231/0.411179/0.293695/0.214726,Accuracy:0.543507, Perplexity: 9.4716\n",
      "Step [681/1584], Loss: 1.9641,BLEU score:0.571210/0.411122/0.293588/0.214624,Accuracy:0.543474, Perplexity: 7.1285\n",
      "Step [691/1584], Loss: 2.0990,BLEU score:0.571160/0.411017/0.293435/0.214513,Accuracy:0.543426, Perplexity: 8.1578\n",
      "Step [701/1584], Loss: 2.0843,BLEU score:0.571154/0.411000/0.293397/0.214450,Accuracy:0.543416, Perplexity: 8.0386\n",
      "Step [711/1584], Loss: 2.1309,BLEU score:0.571266/0.411115/0.293454/0.214453,Accuracy:0.543527, Perplexity: 8.4225\n",
      "Step [721/1584], Loss: 2.1183,BLEU score:0.571172/0.411021/0.293299/0.214234,Accuracy:0.543442, Perplexity: 8.3171\n",
      "Step [731/1584], Loss: 2.1239,BLEU score:0.571302/0.411203/0.293490/0.214398,Accuracy:0.543602, Perplexity: 8.3636\n",
      "Step [741/1584], Loss: 2.0662,BLEU score:0.571352/0.411280/0.293554/0.214477,Accuracy:0.543654, Perplexity: 7.8948\n",
      "Step [751/1584], Loss: 2.1167,BLEU score:0.571380/0.411304/0.293554/0.214509,Accuracy:0.543687, Perplexity: 8.3041\n",
      "Step [761/1584], Loss: 1.9816,BLEU score:0.571389/0.411329/0.293568/0.214530,Accuracy:0.543711, Perplexity: 7.2544\n",
      "Step [771/1584], Loss: 2.1151,BLEU score:0.571423/0.411359/0.293560/0.214585,Accuracy:0.543725, Perplexity: 8.2908\n",
      "Step [781/1584], Loss: 2.1433,BLEU score:0.571406/0.411359/0.293595/0.214647,Accuracy:0.543683, Perplexity: 8.5279\n",
      "Step [791/1584], Loss: 2.0771,BLEU score:0.571461/0.411438/0.293668/0.214715,Accuracy:0.543703, Perplexity: 7.9815\n",
      "Step [801/1584], Loss: 2.0942,BLEU score:0.571475/0.411496/0.293768/0.214792,Accuracy:0.543722, Perplexity: 8.1192\n",
      "Step [811/1584], Loss: 2.0227,BLEU score:0.571566/0.411613/0.293878/0.214873,Accuracy:0.543813, Perplexity: 7.5587\n",
      "Step [821/1584], Loss: 2.2130,BLEU score:0.571499/0.411516/0.293723/0.214692,Accuracy:0.543778, Perplexity: 9.1434\n",
      "Step [831/1584], Loss: 2.0935,BLEU score:0.571496/0.411471/0.293658/0.214666,Accuracy:0.543771, Perplexity: 8.1134\n",
      "Step [841/1584], Loss: 2.1602,BLEU score:0.571530/0.411503/0.293747/0.214766,Accuracy:0.543800, Perplexity: 8.6730\n",
      "Step [851/1584], Loss: 2.0067,BLEU score:0.571542/0.411520/0.293756/0.214763,Accuracy:0.543789, Perplexity: 7.4389\n",
      "Step [861/1584], Loss: 2.1194,BLEU score:0.571569/0.411554/0.293761/0.214776,Accuracy:0.543805, Perplexity: 8.3260\n",
      "Step [871/1584], Loss: 2.2003,BLEU score:0.571577/0.411555/0.293763/0.214838,Accuracy:0.543820, Perplexity: 9.0279\n",
      "Step [881/1584], Loss: 2.0731,BLEU score:0.571604/0.411596/0.293778/0.214877,Accuracy:0.543813, Perplexity: 7.9498\n",
      "Step [891/1584], Loss: 2.1249,BLEU score:0.571622/0.411610/0.293772/0.214864,Accuracy:0.543806, Perplexity: 8.3718\n",
      "Step [901/1584], Loss: 2.0743,BLEU score:0.571594/0.411581/0.293740/0.214839,Accuracy:0.543762, Perplexity: 7.9587\n",
      "Step [911/1584], Loss: 2.0056,BLEU score:0.571597/0.411599/0.293708/0.214838,Accuracy:0.543770, Perplexity: 7.4309\n",
      "Step [921/1584], Loss: 1.9845,BLEU score:0.571552/0.411549/0.293652/0.214731,Accuracy:0.543736, Perplexity: 7.2758\n",
      "Step [931/1584], Loss: 2.0184,BLEU score:0.571528/0.411509/0.293632/0.214686,Accuracy:0.543704, Perplexity: 7.5262\n",
      "Step [941/1584], Loss: 2.1649,BLEU score:0.571404/0.411357/0.293513/0.214601,Accuracy:0.543577, Perplexity: 8.7133\n",
      "Step [951/1584], Loss: 2.1956,BLEU score:0.571420/0.411430/0.293598/0.214686,Accuracy:0.543616, Perplexity: 8.9851\n",
      "Step [961/1584], Loss: 1.9736,BLEU score:0.571456/0.411516/0.293693/0.214767,Accuracy:0.543689, Perplexity: 7.1962\n",
      "Step [971/1584], Loss: 1.9864,BLEU score:0.571539/0.411585/0.293778/0.214864,Accuracy:0.543739, Perplexity: 7.2890\n",
      "Step [981/1584], Loss: 2.0594,BLEU score:0.571486/0.411534/0.293700/0.214804,Accuracy:0.543694, Perplexity: 7.8411\n",
      "Step [991/1584], Loss: 2.0089,BLEU score:0.571434/0.411457/0.293619/0.214714,Accuracy:0.543657, Perplexity: 7.4549\n",
      "Step [1001/1584], Loss: 2.0967,BLEU score:0.571406/0.411418/0.293557/0.214663,Accuracy:0.543640, Perplexity: 8.1390\n",
      "Step [1011/1584], Loss: 2.0896,BLEU score:0.571371/0.411365/0.293504/0.214585,Accuracy:0.543644, Perplexity: 8.0813\n",
      "Step [1021/1584], Loss: 2.0684,BLEU score:0.571374/0.411361/0.293480/0.214552,Accuracy:0.543642, Perplexity: 7.9124\n",
      "Step [1031/1584], Loss: 2.0501,BLEU score:0.571398/0.411417/0.293526/0.214604,Accuracy:0.543682, Perplexity: 7.7683\n",
      "Step [1041/1584], Loss: 2.1706,BLEU score:0.571338/0.411302/0.293333/0.214407,Accuracy:0.543610, Perplexity: 8.7632\n",
      "Step [1051/1584], Loss: 2.0248,BLEU score:0.571380/0.411315/0.293343/0.214349,Accuracy:0.543647, Perplexity: 7.5744\n",
      "Step [1061/1584], Loss: 2.1224,BLEU score:0.571425/0.411337/0.293326/0.214373,Accuracy:0.543670, Perplexity: 8.3508\n",
      "Step [1071/1584], Loss: 2.0024,BLEU score:0.571411/0.411311/0.293296/0.214357,Accuracy:0.543671, Perplexity: 7.4069\n",
      "Step [1081/1584], Loss: 2.2906,BLEU score:0.571355/0.411248/0.293237/0.214230,Accuracy:0.543652, Perplexity: 9.8811\n",
      "Step [1091/1584], Loss: 2.0504,BLEU score:0.571318/0.411235/0.293190/0.214203,Accuracy:0.543637, Perplexity: 7.7713\n",
      "Step [1101/1584], Loss: 2.0298,BLEU score:0.571313/0.411249/0.293212/0.214213,Accuracy:0.543632, Perplexity: 7.6129\n",
      "Step [1111/1584], Loss: 2.1101,BLEU score:0.571313/0.411226/0.293170/0.214165,Accuracy:0.543644, Perplexity: 8.2487\n",
      "Step [1121/1584], Loss: 2.0116,BLEU score:0.571272/0.411177/0.293133/0.214143,Accuracy:0.543598, Perplexity: 7.4754\n",
      "Step [1131/1584], Loss: 2.0893,BLEU score:0.571240/0.411179/0.293158/0.214159,Accuracy:0.543576, Perplexity: 8.0795\n",
      "Step [1141/1584], Loss: 2.1092,BLEU score:0.571239/0.411193/0.293120/0.214126,Accuracy:0.543590, Perplexity: 8.2416\n",
      "Step [1151/1584], Loss: 2.2235,BLEU score:0.571214/0.411161/0.293101/0.214169,Accuracy:0.543550, Perplexity: 9.2396\n",
      "Step [1161/1584], Loss: 2.1493,BLEU score:0.571200/0.411182/0.293131/0.214210,Accuracy:0.543545, Perplexity: 8.5789\n",
      "Step [1171/1584], Loss: 2.0111,BLEU score:0.571202/0.411207/0.293168/0.214242,Accuracy:0.543573, Perplexity: 7.4717\n",
      "Step [1181/1584], Loss: 2.3776,BLEU score:0.571169/0.411166/0.293109/0.214198,Accuracy:0.543539, Perplexity: 10.7792\n",
      "Step [1191/1584], Loss: 1.9816,BLEU score:0.571116/0.411107/0.293022/0.214104,Accuracy:0.543491, Perplexity: 7.2547\n",
      "Step [1201/1584], Loss: 2.2116,BLEU score:0.571129/0.411129/0.293043/0.214094,Accuracy:0.543485, Perplexity: 9.1300\n",
      "Step [1211/1584], Loss: 2.2414,BLEU score:0.571188/0.411185/0.293084/0.214090,Accuracy:0.543511, Perplexity: 9.4066\n",
      "Step [1221/1584], Loss: 1.9718,BLEU score:0.571176/0.411172/0.293070/0.214110,Accuracy:0.543505, Perplexity: 7.1834\n",
      "Step [1231/1584], Loss: 2.2163,BLEU score:0.571115/0.411108/0.293010/0.214108,Accuracy:0.543452, Perplexity: 9.1735\n",
      "Step [1241/1584], Loss: 2.0196,BLEU score:0.571150/0.411148/0.293069/0.214180,Accuracy:0.543479, Perplexity: 7.5350\n",
      "Step [1251/1584], Loss: 2.0506,BLEU score:0.571122/0.411118/0.293036/0.214147,Accuracy:0.543442, Perplexity: 7.7726\n",
      "Step [1261/1584], Loss: 2.0411,BLEU score:0.571125/0.411110/0.293048/0.214184,Accuracy:0.543443, Perplexity: 7.6988\n",
      "Step [1271/1584], Loss: 2.0241,BLEU score:0.571105/0.411063/0.292971/0.214121,Accuracy:0.543414, Perplexity: 7.5692\n",
      "Step [1281/1584], Loss: 1.9968,BLEU score:0.571092/0.411012/0.292905/0.214078,Accuracy:0.543396, Perplexity: 7.3654\n",
      "Step [1291/1584], Loss: 2.1429,BLEU score:0.571069/0.410986/0.292886/0.214068,Accuracy:0.543361, Perplexity: 8.5239\n",
      "Step [1301/1584], Loss: 2.0882,BLEU score:0.571122/0.411058/0.292942/0.214088,Accuracy:0.543430, Perplexity: 8.0701\n",
      "Step [1311/1584], Loss: 2.0205,BLEU score:0.571138/0.411077/0.292908/0.214066,Accuracy:0.543429, Perplexity: 7.5423\n",
      "Step [1321/1584], Loss: 2.1343,BLEU score:0.571178/0.411118/0.292919/0.214101,Accuracy:0.543473, Perplexity: 8.4511\n",
      "Step [1331/1584], Loss: 2.0614,BLEU score:0.571160/0.411120/0.292923/0.214083,Accuracy:0.543466, Perplexity: 7.8568\n",
      "Step [1341/1584], Loss: 2.1168,BLEU score:0.571233/0.411220/0.292994/0.214144,Accuracy:0.543519, Perplexity: 8.3046\n",
      "Step [1351/1584], Loss: 2.1392,BLEU score:0.571235/0.411192/0.292956/0.214150,Accuracy:0.543501, Perplexity: 8.4930\n",
      "Step [1361/1584], Loss: 2.0069,BLEU score:0.571291/0.411239/0.293016/0.214212,Accuracy:0.543538, Perplexity: 7.4404\n",
      "Step [1371/1584], Loss: 1.9078,BLEU score:0.571266/0.411194/0.292970/0.214138,Accuracy:0.543535, Perplexity: 6.7385\n",
      "Step [1381/1584], Loss: 2.0401,BLEU score:0.571277/0.411205/0.292931/0.214121,Accuracy:0.543554, Perplexity: 7.6911\n",
      "Step [1391/1584], Loss: 1.9307,BLEU score:0.571299/0.411212/0.292960/0.214166,Accuracy:0.543582, Perplexity: 6.8940\n",
      "Step [1401/1584], Loss: 2.0916,BLEU score:0.571288/0.411194/0.292938/0.214134,Accuracy:0.543559, Perplexity: 8.0975\n",
      "Step [1411/1584], Loss: 2.1231,BLEU score:0.571296/0.411201/0.292963/0.214163,Accuracy:0.543565, Perplexity: 8.3566\n",
      "Step [1421/1584], Loss: 2.0278,BLEU score:0.571286/0.411169/0.292920/0.214122,Accuracy:0.543556, Perplexity: 7.5970\n",
      "Step [1431/1584], Loss: 2.0430,BLEU score:0.571286/0.411154/0.292924/0.214171,Accuracy:0.543541, Perplexity: 7.7136\n",
      "Step [1441/1584], Loss: 2.1525,BLEU score:0.571284/0.411182/0.292954/0.214169,Accuracy:0.543546, Perplexity: 8.6060\n",
      "Step [1451/1584], Loss: 2.0940,BLEU score:0.571306/0.411200/0.292982/0.214205,Accuracy:0.543560, Perplexity: 8.1175\n",
      "Step [1461/1584], Loss: 2.1029,BLEU score:0.571287/0.411154/0.292935/0.214174,Accuracy:0.543550, Perplexity: 8.1896\n",
      "Step [1471/1584], Loss: 2.0315,BLEU score:0.571308/0.411149/0.292916/0.214162,Accuracy:0.543562, Perplexity: 7.6259\n",
      "Step [1481/1584], Loss: 2.0582,BLEU score:0.571213/0.411057/0.292833/0.214028,Accuracy:0.543475, Perplexity: 7.8316\n",
      "Step [1491/1584], Loss: 2.1446,BLEU score:0.571187/0.411020/0.292787/0.213970,Accuracy:0.543465, Perplexity: 8.5386\n",
      "Step [1501/1584], Loss: 2.2589,BLEU score:0.571218/0.411066/0.292823/0.214004,Accuracy:0.543492, Perplexity: 9.5726\n",
      "Step [1511/1584], Loss: 2.0973,BLEU score:0.571222/0.411079/0.292812/0.214030,Accuracy:0.543491, Perplexity: 8.1438\n",
      "Step [1521/1584], Loss: 2.1180,BLEU score:0.571199/0.411052/0.292785/0.213996,Accuracy:0.543473, Perplexity: 8.3145\n",
      "Step [1531/1584], Loss: 1.9375,BLEU score:0.571203/0.411041/0.292762/0.213993,Accuracy:0.543470, Perplexity: 6.9411\n",
      "Step [1541/1584], Loss: 2.2125,BLEU score:0.571199/0.411038/0.292737/0.213981,Accuracy:0.543469, Perplexity: 9.1385\n",
      "Step [1551/1584], Loss: 2.0847,BLEU score:0.571163/0.411012/0.292736/0.214014,Accuracy:0.543444, Perplexity: 8.0423\n",
      "Step [1561/1584], Loss: 2.2925,BLEU score:0.571108/0.410927/0.292633/0.213933,Accuracy:0.543385, Perplexity: 9.8995\n",
      "Step [1571/1584], Loss: 2.0367,BLEU score:0.571146/0.410967/0.292669/0.214009,Accuracy:0.543400, Perplexity: 7.6657\n",
      "Step [1581/1584], Loss: 2.0509,BLEU score:0.571144/0.410950/0.292646/0.213976,Accuracy:0.543384, Perplexity: 7.7747\n"
     ]
    }
   ],
   "source": [
    "# 这个是 resNext101 + treshold=4 +dropout\n",
    "! python evaluate.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
